---
title: "testing"
author: "Asad Tariq"
date: "`r Sys.Date()`"
output: pdf_document
---

## Loading Libraries

```{r}
library(dplyr)
library(tidyverse)
library(caret)
library(tm)
```

## Reading the Dataset

```{r}
df <- read_csv("../data/fake_job_postings.csv")
head(df)
```


## Extracting Required Columns

```{r}
df_sub <- select(
  df, title, location, department, company_profile, description,
  requirements, benefits, telecommuting, has_company_logo, has_questions,
  employment_type, required_experience, required_education, sub_industry,
  industry, fraudulent
  )

head(df_sub)
```

## Removing the NAs

```{r}
df_sub$benefits <- ifelse(is.na(df_sub$benefits), 0, 1)
df_sub$location[is.na(df_sub$location)] <- "Not-Specified"
df_sub$department[is.na(df_sub$department)] <- "Not-Specified"
df_sub$employment_type[is.na(df_sub$employment_type)] <- "Not-Specified"
df_sub$required_experience[is.na(df_sub$required_experience)] <- "Not-Specified"
df_sub$required_education[is.na(df_sub$required_education)] <- "Not-Specified"
df_sub$sub_industry[is.na(df_sub$sub_industry)] <- "Not-Specified"
df_sub$industry[is.na(df_sub$industry)] <- "Not-Specified"
```

## Further modifications

```{r}
df_sub$location <- sub(",.*", "", df$location)
```

## Constructing a Job Text column

```{r}
df_sub$text <- paste(df_sub$title, df_sub$location, df_sub$company_profile,
                     df_sub$description, df_sub$requirements, sep = " ")
```

## Dropping the unwanted columns

```{r}
df_sub <- subset(df_sub,
                 select = -c(title, location, company_profile, description,
                             requirements))
```

## Building a corpus from the Text column

```{r}
data_corpus <- Corpus(VectorSource(df_sub$text))
data_corpus <- tm_map(data_corpus, removePunctuation)
data_corpus <- tm_map(data_corpus, removeWords, tm::stopwords(kind = "en"))
data_corpus <- tm_map(data_corpus, stripWhitespace)
data_corpus <- tm_map(data_corpus, stemDocument)
```

### Checking the word frequencies

```{r}
frequencies <- DocumentTermMatrix(data_corpus)
```

```{r}
sparse_data <- removeSparseTerms(frequencies, 0.995)
sparse_data_df <- as.data.frame(as.matrix(sparse_data))
colnames(sparse_data_df) <- make.names(colnames(sparse_data_df))
sparse_data_df$fraudulent <- df_sub$fraudulent
```

## Adding features

```{r}
sparse_data_df <- cbind(df_sub$benefits, df_sub$telecommuting,
                       df_sub$has_company_logo, df_sub$has_questions,
                       sparse_data_df)

colnames(sparse_data_df)[1:4] <- c("benefits", "telecommuting",
                                   "has_company_logo", "has_questions")
```

```{r}
names(sparse_data_df) <- gsub("\\.$", "", names(sparse_data_df))
```

```{r}
basic_logit <- glm("fraudulent ~ has_company_logo + has_questions + 
                   benefits + multitask + customer*services",
                   data = sparse_data_df,
                   family = "binomial")
summary(basic_logit)
```

```{r}
library(randomForest)
library(caret)
```


```{r}

set.seed(123)
train_index <- createDataPartition(sparse_data_df$fraudulent, p = 0.7, list = FALSE)
train_data <- sparse_data_df[train_index, ]
test_data <- sparse_data_df[-train_index, ]

train_data$fraudulent <- as.factor(train_data$fraudulent)
test_data$fraudulent <- as.factor(test_data$fraudulent)

str(train_data[, c("has_company_logo", "has_questions", "benefits", "multitask", "customer", "services")])

```


```{r}
# Assuming your target variable is called 'fraudulent' and it's a factor
rf_model <- randomForest(fraudulent ~ has_company_logo + has_questions + benefits, data = train_data, ntree = 100, importance = TRUE)
```

```{r}
#make predictions
rf_predictions <- predict(rf_model, newdata = test_data)

# Confusion Matrix using caret
conf_matrix <- confusionMatrix(data = rf_predictions,
                               reference = test_data$fraudulent,
                               positive = "1",
                               mode = "everything")

print(conf_matrix)

# Accuracy
accuracy <- conf_matrix$overall["Accuracy"]
cat("Accuracy:", round(accuracy, 4))


```

```{r}
library(dplyr)
train_data <- train_data[, !duplicated(names(train_data))]

# 1. Split the training data by class
fraudulent_jobs <- train_data %>% filter(fraudulent == "1")
nonfraudulent_jobs <- train_data %>% filter(fraudulent == "0")

# 2. Oversample the minority class to match the majority
oversampled_fraud <- fraudulent_jobs %>%
  slice_sample(n = nrow(nonfraudulent_jobs), replace = TRUE)

# 3. Combine into balanced training set
train_data_balanced <- bind_rows(nonfraudulent_jobs, oversampled_fraud)

# 4. Shuffle the rows
train_data_balanced <- train_data_balanced %>%
  slice_sample(n = nrow(train_data_balanced))

# Optional: confirm balance
table(train_data_balanced$fraudulent)

```

```{r}
rf_model <- randomForest(
  fraudulent ~ has_company_logo + has_questions + benefits + multitask + customer * services,
  data = train_data_balanced,
  ntree = 100,
  importance = TRUE
)

# Predict on the original test set
rf_predictions <- predict(rf_model, newdata = test_data)

# Evaluate performance
conf_matrix <- confusionMatrix(
  data = rf_predictions,
  reference = test_data$fraudulent,
  positive = "1",
  mode = "everything"
)

print(conf_matrix)
cat("Accuracy:", round(conf_matrix$overall["Accuracy"], 4))
```





